<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding TF-IDF (Term Frequency-Inverse Document Frequency) | Blogs</title>
<meta name=keywords content="NLP,TF-IDF"><meta name=description content="Introduction
When dealing with large collections of text, such as in search engines, recommendation systems, or natural language processing (NLP) tasks, it&rsquo;s important to quantify the relevance of words in documents. One of the most popular methods to do this is TF-IDF, which stands for Term Frequency-Inverse Document Frequency.
TF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents, known as the corpus. The underlying principle of TF-IDF is that words that occur frequently in a document but not across the entire corpus are more important. Let&rsquo;s break this down."><meta name=author content><link rel=canonical href=https://harshu6397.github.io/my-blog-website/posts/2024-09-30-tf-idf/><link crossorigin=anonymous href=/my-blog-website/assets/css/stylesheet.d507d6822eb9a3efb53a13a0a12cb519a8994001172225bf31bd29b2275b3305.css integrity="sha256-1QfWgi65o++1OhOgoSy1GaiZQAEXIiW/Mb0psidbMwU=" rel="preload stylesheet" as=style><link rel=icon href=https://harshu6397.github.io/my-blog-website/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://harshu6397.github.io/my-blog-website/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://harshu6397.github.io/my-blog-website/favicon-32x32.png><link rel=apple-touch-icon href=https://harshu6397.github.io/my-blog-website/apple-touch-icon.png><link rel=mask-icon href=https://harshu6397.github.io/my-blog-website/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://harshu6397.github.io/my-blog-website/posts/2024-09-30-tf-idf/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Understanding TF-IDF (Term Frequency-Inverse Document Frequency)"><meta property="og:description" content="Introduction
When dealing with large collections of text, such as in search engines, recommendation systems, or natural language processing (NLP) tasks, it&rsquo;s important to quantify the relevance of words in documents. One of the most popular methods to do this is TF-IDF, which stands for Term Frequency-Inverse Document Frequency.
TF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents, known as the corpus. The underlying principle of TF-IDF is that words that occur frequently in a document but not across the entire corpus are more important. Let&rsquo;s break this down."><meta property="og:type" content="article"><meta property="og:url" content="https://harshu6397.github.io/my-blog-website/posts/2024-09-30-tf-idf/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-30T17:03:54+05:30"><meta property="article:modified_time" content="2024-09-30T17:03:54+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding TF-IDF (Term Frequency-Inverse Document Frequency)"><meta name=twitter:description content="Introduction
When dealing with large collections of text, such as in search engines, recommendation systems, or natural language processing (NLP) tasks, it&rsquo;s important to quantify the relevance of words in documents. One of the most popular methods to do this is TF-IDF, which stands for Term Frequency-Inverse Document Frequency.
TF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents, known as the corpus. The underlying principle of TF-IDF is that words that occur frequently in a document but not across the entire corpus are more important. Let&rsquo;s break this down."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://harshu6397.github.io/my-blog-website/posts/"},{"@type":"ListItem","position":2,"name":"Understanding TF-IDF (Term Frequency-Inverse Document Frequency)","item":"https://harshu6397.github.io/my-blog-website/posts/2024-09-30-tf-idf/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding TF-IDF (Term Frequency-Inverse Document Frequency)","name":"Understanding TF-IDF (Term Frequency-Inverse Document Frequency)","description":"Introduction When dealing with large collections of text, such as in search engines, recommendation systems, or natural language processing (NLP) tasks, it\u0026rsquo;s important to quantify the relevance of words in documents. One of the most popular methods to do this is TF-IDF, which stands for Term Frequency-Inverse Document Frequency.\nTF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents, known as the corpus. The underlying principle of TF-IDF is that words that occur frequently in a document but not across the entire corpus are more important. Let\u0026rsquo;s break this down.\n","keywords":["NLP","TF-IDF"],"articleBody":"Introduction When dealing with large collections of text, such as in search engines, recommendation systems, or natural language processing (NLP) tasks, it’s important to quantify the relevance of words in documents. One of the most popular methods to do this is TF-IDF, which stands for Term Frequency-Inverse Document Frequency.\nTF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents, known as the corpus. The underlying principle of TF-IDF is that words that occur frequently in a document but not across the entire corpus are more important. Let’s break this down.\nKey Terms Document: A document refers to a single unit of text, such as an article, email, or webpage. It is part of a larger collection of documents. Corpus: A corpus (plural: corpora) is a collection of documents. In TF-IDF, the corpus is used to determine how common or rare a term is across many documents. Term (or Token): A term or token refers to any individual word in a document. In TF-IDF, we calculate the importance of each term within the document and the corpus. 1. Term Frequency (TF) The Term Frequency (TF) is a measure of how often a term appears in a specific document. It helps identify the importance of a term in the context of that particular document.\nFormula: TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d) Example: If the term “computer” appears 10 times in a document that has 100 total words, the TF of “computer” is:\nTF(\"computer\") = 10 / 100 = 0.1 The higher the frequency, the more important the word is to the document.\n2. Inverse Document Frequency (IDF) The Inverse Document Frequency (IDF) measures how important a term is across the entire corpus. A term that appears in many documents is considered less unique, and its IDF value will be lower. Conversely, a term that appears in only a few documents will have a higher IDF, reflecting its uniqueness and importance.\nFormula: IDF(t) = log(Total number of documents / Number of documents containing term t) Example: If the word “computer” appears in 3 out of 100 documents, its IDF is:\nIDF(\"computer\") = log(100 / 3) ≈ 1.52 A high IDF indicates that the term is rare across the corpus, making it more meaningful.\n3. The TF-IDF Score The TF-IDF score combines both the term frequency and inverse document frequency. It is calculated by multiplying TF and IDF together:\nTF-IDF(t, d) = TF(t, d) * IDF(t) This score indicates the importance of a term in a specific document, while accounting for its rarity across the entire corpus.\nExample: Using the previous values, if the TF of “computer” is 0.1 and the IDF is 1.52, the TF-IDF score is:\nTF-IDF(\"computer\") = 0.1 * 1.52 = 0.152 Thus, “computer” is relatively important in the document but not so common across all documents.\nKey Insights High TF-IDF: A high score means the term is important in the document but rare across the corpus. Low TF-IDF: A low score could mean the term is either common across documents or not particularly relevant in the specific document. Applications of TF-IDF TF-IDF is widely used in several real-world applications:\nSearch Engines: TF-IDF is used to rank pages based on the relevance of search queries. Text Mining: It helps in identifying key terms in a large set of documents. Recommendation Systems: TF-IDF can be used to suggest articles, books, or other documents that match user preferences. Sentiment Analysis: It helps identify which terms contribute the most to the sentiment of a document. Final Thoughts TF-IDF is a powerful and widely used technique for quantifying the relevance of terms in documents. By combining term frequency with the rarity of terms across documents, it provides a more accurate measure of importance. Whether you’re building a search engine or analyzing large datasets, understanding how TF-IDF works can be invaluable.\n","wordCount":"660","inLanguage":"en","datePublished":"2024-09-30T17:03:54+05:30","dateModified":"2024-09-30T17:03:54+05:30","mainEntityOfPage":{"@type":"WebPage","@id":"https://harshu6397.github.io/my-blog-website/posts/2024-09-30-tf-idf/"},"publisher":{"@type":"Organization","name":"Blogs","logo":{"@type":"ImageObject","url":"https://harshu6397.github.io/my-blog-website/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://harshu6397.github.io/my-blog-website/ accesskey=h title="Blogs (Alt + H)">Blogs</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://harshu6397.github.io/my-blog-website/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://harshu6397.github.io/my-blog-website/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://harshu6397.github.io/my-blog-website/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://harshu6397.github.io/my-blog-website/>Home</a>&nbsp;»&nbsp;<a href=https://harshu6397.github.io/my-blog-website/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Understanding TF-IDF (Term Frequency-Inverse Document Frequency)</h1><div class=post-meta><span title='2024-09-30 17:03:54 +0530 IST'>September 30, 2024</span>&nbsp;·&nbsp;4 min</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a><ul><li><a href=#key-terms aria-label="Key Terms">Key Terms</a><ul><li><a href=#document aria-label=Document:>Document:</a></li><li><a href=#corpus aria-label=Corpus:>Corpus:</a></li><li><a href=#term-or-token aria-label="Term (or Token):">Term (or Token):</a></li></ul></li></ul></li><li><a href=#1-term-frequency-tf aria-label="1. Term Frequency (TF)">1. Term Frequency (TF)</a><ul><li><a href=#formula aria-label=Formula:>Formula:</a></li><li><a href=#example aria-label=Example:>Example:</a></li></ul></li><li><a href=#2-inverse-document-frequency-idf aria-label="2. Inverse Document Frequency (IDF)">2. Inverse Document Frequency (IDF)</a><ul><li><a href=#formula-1 aria-label=Formula:>Formula:</a></li><li><a href=#example-1 aria-label=Example:>Example:</a></li></ul></li><li><a href=#3-the-tf-idf-score aria-label="3. The TF-IDF Score">3. The TF-IDF Score</a><ul><li><a href=#example-2 aria-label=Example:>Example:</a></li></ul></li><li><a href=#key-insights aria-label="Key Insights">Key Insights</a></li><li><a href=#applications-of-tf-idf aria-label="Applications of TF-IDF">Applications of TF-IDF</a></li><li><a href=#final-thoughts aria-label="Final Thoughts">Final Thoughts</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>When dealing with large collections of text, such as in search engines, recommendation systems, or natural language processing (NLP) tasks, it&rsquo;s important to quantify the relevance of words in documents. One of the most popular methods to do this is <strong>TF-IDF</strong>, which stands for <strong>Term Frequency-Inverse Document Frequency</strong>.</p><p>TF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents, known as the corpus. The underlying principle of TF-IDF is that words that occur frequently in a document but not across the entire corpus are more important. Let&rsquo;s break this down.</p><h3 id=key-terms>Key Terms<a hidden class=anchor aria-hidden=true href=#key-terms>#</a></h3><h4 id=document>Document:<a hidden class=anchor aria-hidden=true href=#document>#</a></h4><ul><li>A <strong>document</strong> refers to a single unit of text, such as an article, email, or webpage. It is part of a larger collection of documents.</li></ul><h4 id=corpus>Corpus:<a hidden class=anchor aria-hidden=true href=#corpus>#</a></h4><ul><li>A <strong>corpus</strong> (plural: <strong>corpora</strong>) is a collection of documents. In TF-IDF, the corpus is used to determine how common or rare a term is across many documents.</li></ul><h4 id=term-or-token>Term (or Token):<a hidden class=anchor aria-hidden=true href=#term-or-token>#</a></h4><ul><li>A <strong>term</strong> or <strong>token</strong> refers to any individual word in a document. In TF-IDF, we calculate the importance of each term within the document and the corpus.</li></ul><h2 id=1-term-frequency-tf>1. Term Frequency (TF)<a hidden class=anchor aria-hidden=true href=#1-term-frequency-tf>#</a></h2><p>The <strong>Term Frequency (TF)</strong> is a measure of how often a term appears in a specific document. It helps identify the importance of a term in the context of that particular document.</p><h3 id=formula>Formula:<a hidden class=anchor aria-hidden=true href=#formula>#</a></h3><pre tabindex=0><code>TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)
</code></pre><h3 id=example>Example:<a hidden class=anchor aria-hidden=true href=#example>#</a></h3><p>If the term &ldquo;computer&rdquo; appears 10 times in a document that has 100 total words, the TF of &ldquo;computer&rdquo; is:</p><pre tabindex=0><code>TF(&#34;computer&#34;) = 10 / 100 = 0.1
</code></pre><p>The higher the frequency, the more important the word is to the document.</p><h2 id=2-inverse-document-frequency-idf>2. Inverse Document Frequency (IDF)<a hidden class=anchor aria-hidden=true href=#2-inverse-document-frequency-idf>#</a></h2><p>The <strong>Inverse Document Frequency (IDF)</strong> measures how important a term is across the entire corpus. A term that appears in many documents is considered less unique, and its IDF value will be lower. Conversely, a term that appears in only a few documents will have a higher IDF, reflecting its uniqueness and importance.</p><h3 id=formula-1>Formula:<a hidden class=anchor aria-hidden=true href=#formula-1>#</a></h3><pre tabindex=0><code>IDF(t) = log(Total number of documents / Number of documents containing term t)
</code></pre><h3 id=example-1>Example:<a hidden class=anchor aria-hidden=true href=#example-1>#</a></h3><p>If the word &ldquo;computer&rdquo; appears in 3 out of 100 documents, its IDF is:</p><pre tabindex=0><code>IDF(&#34;computer&#34;) = log(100 / 3) ≈ 1.52
</code></pre><p>A high IDF indicates that the term is rare across the corpus, making it more meaningful.</p><h2 id=3-the-tf-idf-score>3. The TF-IDF Score<a hidden class=anchor aria-hidden=true href=#3-the-tf-idf-score>#</a></h2><p>The <strong>TF-IDF score</strong> combines both the term frequency and inverse document frequency. It is calculated by multiplying TF and IDF together:</p><pre tabindex=0><code>TF-IDF(t, d) = TF(t, d) * IDF(t)
</code></pre><p>This score indicates the importance of a term in a specific document, while accounting for its rarity across the entire corpus.</p><h3 id=example-2>Example:<a hidden class=anchor aria-hidden=true href=#example-2>#</a></h3><p>Using the previous values, if the TF of &ldquo;computer&rdquo; is 0.1 and the IDF is 1.52, the TF-IDF score is:</p><pre tabindex=0><code>TF-IDF(&#34;computer&#34;) = 0.1 * 1.52 = 0.152
</code></pre><p>Thus, &ldquo;computer&rdquo; is relatively important in the document but not so common across all documents.</p><h2 id=key-insights>Key Insights<a hidden class=anchor aria-hidden=true href=#key-insights>#</a></h2><ul><li><strong>High TF-IDF</strong>: A high score means the term is important in the document but rare across the corpus.</li><li><strong>Low TF-IDF</strong>: A low score could mean the term is either common across documents or not particularly relevant in the specific document.</li></ul><h2 id=applications-of-tf-idf>Applications of TF-IDF<a hidden class=anchor aria-hidden=true href=#applications-of-tf-idf>#</a></h2><p>TF-IDF is widely used in several real-world applications:</p><ul><li><strong>Search Engines</strong>: TF-IDF is used to rank pages based on the relevance of search queries.</li><li><strong>Text Mining</strong>: It helps in identifying key terms in a large set of documents.</li><li><strong>Recommendation Systems</strong>: TF-IDF can be used to suggest articles, books, or other documents that match user preferences.</li><li><strong>Sentiment Analysis</strong>: It helps identify which terms contribute the most to the sentiment of a document.</li></ul><h2 id=final-thoughts>Final Thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h2><p>TF-IDF is a powerful and widely used technique for quantifying the relevance of terms in documents. By combining term frequency with the rarity of terms across documents, it provides a more accurate measure of importance. Whether you’re building a search engine or analyzing large datasets, understanding how TF-IDF works can be invaluable.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://harshu6397.github.io/my-blog-website/tags/nlp/>NLP</a></li><li><a href=https://harshu6397.github.io/my-blog-website/tags/tf-idf/>TF-IDF</a></li></ul><nav class=paginav><a class=next href=https://harshu6397.github.io/my-blog-website/posts/2024-07-12-sql-joins/><span class=title>Next »</span><br><span>SQL Joins</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding TF-IDF (Term Frequency-Inverse Document Frequency) on x" href="https://x.com/intent/tweet/?text=Understanding%20TF-IDF%20%28Term%20Frequency-Inverse%20Document%20Frequency%29&amp;url=https%3a%2f%2fharshu6397.github.io%2fmy-blog-website%2fposts%2f2024-09-30-tf-idf%2f&amp;hashtags=NLP%2cTF-IDF"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding TF-IDF (Term Frequency-Inverse Document Frequency) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fharshu6397.github.io%2fmy-blog-website%2fposts%2f2024-09-30-tf-idf%2f&amp;title=Understanding%20TF-IDF%20%28Term%20Frequency-Inverse%20Document%20Frequency%29&amp;summary=Understanding%20TF-IDF%20%28Term%20Frequency-Inverse%20Document%20Frequency%29&amp;source=https%3a%2f%2fharshu6397.github.io%2fmy-blog-website%2fposts%2f2024-09-30-tf-idf%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding TF-IDF (Term Frequency-Inverse Document Frequency) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fharshu6397.github.io%2fmy-blog-website%2fposts%2f2024-09-30-tf-idf%2f&title=Understanding%20TF-IDF%20%28Term%20Frequency-Inverse%20Document%20Frequency%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding TF-IDF (Term Frequency-Inverse Document Frequency) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fharshu6397.github.io%2fmy-blog-website%2fposts%2f2024-09-30-tf-idf%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding TF-IDF (Term Frequency-Inverse Document Frequency) on whatsapp" href="https://api.whatsapp.com/send?text=Understanding%20TF-IDF%20%28Term%20Frequency-Inverse%20Document%20Frequency%29%20-%20https%3a%2f%2fharshu6397.github.io%2fmy-blog-website%2fposts%2f2024-09-30-tf-idf%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding TF-IDF (Term Frequency-Inverse Document Frequency) on telegram" href="https://telegram.me/share/url?text=Understanding%20TF-IDF%20%28Term%20Frequency-Inverse%20Document%20Frequency%29&amp;url=https%3a%2f%2fharshu6397.github.io%2fmy-blog-website%2fposts%2f2024-09-30-tf-idf%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding TF-IDF (Term Frequency-Inverse Document Frequency) on ycombinator" href="https://news.ycombinator.com/submitlink?t=Understanding%20TF-IDF%20%28Term%20Frequency-Inverse%20Document%20Frequency%29&u=https%3a%2f%2fharshu6397.github.io%2fmy-blog-website%2fposts%2f2024-09-30-tf-idf%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://harshu6397.github.io/my-blog-website/>Blogs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>